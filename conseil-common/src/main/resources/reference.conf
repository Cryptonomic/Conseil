# Default global configuration file. Should be overriden for full functionality.

# Configuration for updating conseil api keys with the ones generated by nautilus-cloud
nautilus-cloud {
  enabled: false
  enabled: ${?CONSEIL_NC_ENABLED}
  host: "http://localhost"
  host: ${?CONSEIL_NC_HOST}
  port: 1234
  port: ${?CONSEIL_NC_PORT}
  path: "apiKeys/dev" // here should be an environment name after '/'
  path: ${?CONSEIL_NC_PATH}
  key: "exampleApiKeyDev"
  key: ${?CONSEIL_NC_KEY}
  delay: 10 seconds
  interval: 30 seconds
}

# Slick debug level
logger.scala.slick = ALL # Configure in logback.xml

# Settings for blockchains Conseil can connect to. Nested by blockchain name and network.
platforms: [
  {
    name: "tezos"

    baker-rolls-size: 8000
    baker-rolls-size: ${?CONSEIL_XTZ_BACKER_ROLLS_SIZE}

    network: "granadanet"
    network: ${?CONSEIL_XTZ_NETWORK}

    enabled: true
    enabled: ${?CONSEIL_XTZ_ENABLED}

    node: {
      protocol: "https"
      protocol: ${?CONSEIL_XTZ_NODE_PROTOCOL}

      hostname: "tezos-granada.cryptonomic-infra.tech"
      hostname: ${?CONSEIL_XTZ_NODE_HOSTNAME}

      port: 443
      port: ${?CONSEIL_XTZ_NODE_PORT}

      path-prefix: ""
      path-prefix: ${?CONSEIL_XTZ_NODE_PATH_PREFIX}

      # allows to trace calls made to the tezos node
      trace-calls: false
      trace-calls: ${?CONSEIL_XTZ_NODE_TRACE_CALLS}
    }

    db {
      dataSourceClass: "org.postgresql.ds.PGSimpleDataSource"

      properties {
        user: "conseiluser"
        user: ${?CONSEIL_XTZ_DB_USER}

        password: "p@ssw0rd"
        password: ${?CONSEIL_XTZ_DB_PASSWORD}

        url: "jdbc:postgresql://localhost:5432/conseil-local"
        url: ${?CONSEIL_XTZ_DB_URL}

        reWriteBatchedInserts: true
      }
      # The following numbers are based on literature from here: https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing
      # We might want to fine-tune these on the actual infrastructure, doing testing with different values
      # Please keep both values aligned in your configuration to avoid this issue: https://github.com/dnvriend/akka-persistence-jdbc/issues/177
      numThreads: 10
      maxConnections: 10
    }
  },
  {
    name: "bitcoin"

    network: "mainnet"
    network: ${?CONSEIL_BTC_NETWORK}

    enabled: false
    enabled: ${?CONSEIL_BTC_ENABLED}

    node: {
      protocol: "http"
      protocol: ${?CONSEIL_BTC_NODE_PROTOCOL}

      hostname: "localhost"
      hostname: ${?CONSEIL_BTC_NODE_HOSTNAME}

      port: 8332
      port: ${?CONSEIL_BTC_NODE_PORT}

      username: "bitcoin"
      username: ${?CONSEIL_BTC_NODE_USERNAME}

      # the password used in local Bitcoin node from the docker-compose.yml file
      password: "sP9PV88xtbGwLMAEi2rVlZ7jIFfJbpOmTUCsBBBRN9I="
      password: ${?CONSEIL_BTC_NODE_PASSWORD}
    }

    db {
      dataSourceClass: "org.postgresql.ds.PGSimpleDataSource"

      properties {
        user: "conseiluser"
        user: ${?CONSEIL_BTC_DB_USER}

        password: "p@ssw0rd"
        password: ${?CONSEIL_BTC_DB_PASSWORD}

        url: "jdbc:postgresql://localhost:5432/conseil-local"
        url: ${?CONSEIL_BTC_DB_URL}

        reWriteBatchedInserts: true
      }
      # The following numbers are based on literature from here: https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing
      # We might want to fine-tune these on the actual infrastructure, doing testing with different values
      # Please keep both values aligned in your configuration to avoid this issue: https://github.com/dnvriend/akka-persistence-jdbc/issues/177
      numThreads: 10
      maxConnections: 10
    }

    batching {
      indexer-threads-count: 8
      indexer-threads-count: ${?CONSEIL_BTC_BATCHING_INDEXER_THREADS_COUNT}

      http-fetch-threads-count: 8
      http-fetch-threads-count: ${?CONSEIL_BTC_BATCHING_HTTP_FETCH_THREADS_COUNT}

      hash-batch-size: 2000
      hash-batch-size: ${?CONSEIL_BTC_BATCHING_HASH_BATCH_SIZE}

      blocks-batch-size: 500
      blocks-batch-size: ${?CONSEIL_BTC_BATCHING_BLOCKS_BATCH_SIZE}

      transactions-batch-size: 500
      transactions-batch-size: ${?CONSEIL_BTC_BATCHING_TRANSACTIONS_BATCH_SIZE}
    }
  },
  {
    name: "ethereum"

    network: "mainnet"
    network: ${?CONSEIL_ETH_NETWORK}

    enabled: false
    enabled: ${?CONSEIL_ETH_ENABLED}

    node: "https://localhost:8332"
    node: ${?CONSEIL_ETH_NODE}

    db {
      dataSourceClass: "org.postgresql.ds.PGSimpleDataSource"

      properties {
        user: "conseiluser"
        user: ${?CONSEIL_ETH_DB_USER}

        password: "p@ssw0rd"
        password: ${?CONSEIL_ETH_DB_PASSWORD}

        url: "jdbc:postgresql://localhost:5432/conseil-local"
        url: ${?CONSEIL_ETH_DB_URL}

        reWriteBatchedInserts: true
      }
      # The following numbers are based on literature from here: https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing
      # We might want to fine-tune these on the actual infrastructure, doing testing with different values
      # Please keep both values aligned in your configuration to avoid this issue: https://github.com/dnvriend/akka-persistence-jdbc/issues/177
      numThreads: 10
      maxConnections: 10
    }

    retry {
      max-wait: 2s
      max-wait: ${?CONSEIL_ETH_RETRY_MAX_WAIT}

      max-retry: 5
      max-retry: ${?CONSEIL_ETH_RETRY_MAX_RETRY}
    }

    batching {
      indexer-threads-count: 8
      indexer-threads-count: ${?CONSEIL_ETH_BATCHING_INDEXER_THREADS_COUNT}

      http-fetch-threads-count: 8
      http-fetch-threads-count: ${?CONSEIL_ETH_BATCHING_HTTP_FETCH_THREADS_COUNT}

      blocks-batch-size: 100
      blocks-batch-size: ${?CONSEIL_ETH_BATCHING_BLOCKS_BATCH_SIZE}

      transactions-batch-size: 100
      transactions-batch-size: ${?CONSEIL_ETH_BATCHING_TRANSACTIONS_BATCH_SIZE}

      contracts-batch-size: 100
      contracts-batch-size: ${?CONSEIL_ETH_BATCHING_CONTRACTS_BATCH_SIZE}

      tokens-batch-size: 100
      tokens-batch-size: ${?CONSEIL_ETH_BATCHING_TOKENS_BATCH_SIZE}
    }
  },
  {
    name: "quorum"

    network: "mainnet"
    network: ${?CONSEIL_QUO_NETWORK}

    enabled: false
    enabled: ${?CONSEIL_QUO_ENABLED}

    node: "https://localhost:8332"
    node: ${?CONSEIL_QUO_NODE}

    db {
      dataSourceClass: "org.postgresql.ds.PGSimpleDataSource"

      properties {
        user: "foo"
        user: ${?CONSEIL_QUO_DB_USER}

        password: "bar"
        password: ${?CONSEIL_QUO_DB_PASSWORD}

        url: "jdbc:postgresql://localhost:5432/postgres"
        url: ${?CONSEIL_QUO_DB_URL}

        reWriteBatchedInserts: true
      }
      # The following numbers are based on literature from here: https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing
      # We might want to fine-tune these on the actual infrastructure, doing testing with different values
      # Please keep both values aligned in your configuration to avoid this issue: https://github.com/dnvriend/akka-persistence-jdbc/issues/177
      numThreads: 10
      maxConnections: 10
    }

    retry {
      max-wait: 2s
      max-wait: ${?CONSEIL_QUO_RETRY_MAX_WAIT}

      max-retry: 5
      max-retry: ${?CONSEIL_QUO_RETRY_MAX_RETRY}
    }

    batching {
      indexer-threads-count: 8
      indexer-threads-count: ${?CONSEIL_QUO_BATCHING_INDEXER_THREADS_COUNT}

      http-fetch-threads-count: 8
      http-fetch-threads-count: ${?CONSEIL_QUO_BATCHING_HTTP_FETCH_THREADS_COUNT}

      blocks-batch-size: 100
      blocks-batch-size: ${?CONSEIL_QUO_BATCHING_BLOCKS_BATCH_SIZE}

      transactions-batch-size: 100
      transactions-batch-size: ${?CONSEIL_QUO_BATCHING_TRANSACTIONS_BATCH_SIZE}

      contracts-batch-size: 100
      contracts-batch-size: ${?CONSEIL_QUO_BATCHING_CONTRACTS_BATCH_SIZE}

      tokens-batch-size: 100
      tokens-batch-size: ${?CONSEIL_QUO_BATCHING_TOKENS_BATCH_SIZE}
    }
  }
]

# Define here a known tezos naming service contract for each network of interest
tns {
  # an example definition might look like
  #
  # mainnet {
  #   name: "TNS name"
  #   contract-type: "TNS"
  #   account-id: "<KT1 address of the contract>"
  # }
}

# Customization on the underlying actor system
akka {

  # custom host pool for akka-http client connections used for streaming request/responses
  # tune the configuration based on load-handling capability of tezos nodes
  # refer to host-connection-pool section in
  # https://doc.akka.io/docs/akka-http/current/configuration.html
  # for available properties and their meaning
  #
  # The current configuration is based on local benchmarking against zeronet
  # On the streaming http client pool we expect a max of:
  #   45 connections x
  #    7 requests/conn ~=
  #  315 ongoing requests at each moment
  # The pipelining on each connection might slow down for slow responses, but they should be rare
  streaming-client {
    max-connections: 45
    # This limit is overestimated by a factor of roughly 10x, to allow room for
    # reuse of the same pool from different threads at the same time, up to that factor
    max-open-requests: 4096
    # essentially keep connections alive across lorre's cycles
    idle-timeout: 10 minutes
    pipelining-limit: 7
    # give more room for async response in head-of-line blocking on the same connection or other slow responses
    response-entity-subscription-timeout: 60 seconds
  }

  # fixes the issue when lorre does not quit after system.shutdown()
  coordinated-shutdown.run-by-actor-system-terminate = off
  http.parsing.max-to-strict-bytes: 100m
  http.parsing.max-to-strict-bytes: ${?CONSEIL_LORRE_AKKA_MAX_TO_STRICT_BYTES}
}

# Custom libSodium settings
sodium.library-path: "/usr/lib/x86_64-linux-gnu/libsodium.so.18"